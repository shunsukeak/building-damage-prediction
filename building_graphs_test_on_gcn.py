import os
import torch
import torch.nn as nn
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from tqdm import tqdm
from collections import defaultdict
import numpy as np

# === ÂÖ•Âá∫Âäõ„Éë„Çπ ===
graph_dir = "./graphs_test_only"
model_dir = "./gcn_models_by_type"

# === GCN„É¢„Éá„É´ÂÆöÁæ© ===
class GCNClassifier(nn.Module):
    def __init__(self, in_dim=518, hidden_dim=128, out_dim=4):
        super().__init__()
        self.gcn1 = GCNConv(in_dim, hidden_dim)
        self.gcn2 = GCNConv(hidden_dim, hidden_dim)
        self.mlp = nn.Sequential(
            nn.ReLU(),
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Linear(64, out_dim)
        )

    def forward(self, x, edge_index):
        x = self.gcn1(x, edge_index)
        x = self.gcn2(x, edge_index)
        return self.mlp(x)

# === Ë©ï‰æ°Èñ¢Êï∞ ===
def evaluate():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    all_preds, all_labels = [], []

    type_pred_dict = defaultdict(list)
    type_label_dict = defaultdict(list)

    for fname in os.listdir(graph_dir):
        if not fname.endswith("_test.pt"):
            continue
        dtype = fname.replace("graph_", "").replace("_test.pt", "")
        print(f"\nüß™ Evaluating test graph for disaster type: {dtype}")

        graph_path = os.path.join(graph_dir, fname)
        model_path = os.path.join(model_dir, f"gcn_{dtype}.pt")
        if not os.path.exists(model_path):
            print(f"‚ö†Ô∏è Model not found for {dtype}, skipping.")
            continue

        g = torch.load(graph_path, weights_only=False)
        model = GCNClassifier().to(device)
        model.load_state_dict(torch.load(model_path, map_location=device))
        model.eval()

        x = g.x.to(device)
        edge_index = g.edge_index.to(device)
        y_true = g.y.cpu().numpy()

        with torch.no_grad():
            out = model(x, edge_index)
            y_pred = out.argmax(dim=1).cpu().numpy()

        acc = accuracy_score(y_true, y_pred)
        print(f"‚úÖ Accuracy: {acc:.4f}")

        all_preds.extend(y_pred)
        all_labels.extend(y_true)
        type_pred_dict[dtype].extend(y_pred)
        type_label_dict[dtype].extend(y_true)

    # === ÂÖ®‰ΩìË©ï‰æ° ===
    if len(all_preds) == 0:
        print("‚ö†Ô∏è No predictions made.")
        return

    print("\nüìä Overall GCN Test Evaluation (test-only graphs):")
    print("‚úÖ Accuracy:", accuracy_score(all_labels, all_preds))
    print("‚úÖ Confusion Matrix:\n", confusion_matrix(all_labels, all_preds))
    print("‚úÖ Classification Report:\n",
          classification_report(all_labels, all_preds, target_names=["no-damage", "minor", "major", "destroyed"]))

    # === ÁÅΩÂÆ≥„Çø„Ç§„ÉóÂçò‰ΩçÈõÜË®à ===
    print("\nüåç Disaster-type summary:")
    disaster_accs = []
    for dtype in sorted(type_pred_dict.keys()):
        preds = np.array(type_pred_dict[dtype])
        labels = np.array(type_label_dict[dtype])
        correct = (preds == labels).sum()
        total = len(labels)
        disaster_acc = correct / total
        disaster_accs.append(disaster_acc)
        print(f"üåç {dtype}: Accuracy = {disaster_acc:.4f} ({correct}/{total})")

    mean_disaster_acc = np.mean(disaster_accs)
    print(f"\nüìä Mean Disaster-type Accuracy: {mean_disaster_acc:.4f}")

# === ÂÆüË°å ===
if __name__ == "__main__":
    evaluate()

# import os
# import torch
# import torch.nn as nn
# from torch_geometric.data import Data
# from torch_geometric.nn import GCNConv
# from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
# from tqdm import tqdm

# # === ÂÖ•Âá∫Âäõ„Éë„Çπ ===
# graph_dir = "./graphs_test_only"
# model_dir = "./gcn_models_by_type"

# # === GCN„É¢„Éá„É´ÂÆöÁæ© ===
# class GCNClassifier(nn.Module):
#     def __init__(self, in_dim=518, hidden_dim=128, out_dim=4):
#         super().__init__()
#         self.gcn1 = GCNConv(in_dim, hidden_dim)
#         self.gcn2 = GCNConv(hidden_dim, hidden_dim)
#         self.mlp = nn.Sequential(
#             nn.ReLU(),
#             nn.Linear(hidden_dim, 64),
#             nn.ReLU(),
#             nn.Linear(64, out_dim)
#         )

#     def forward(self, x, edge_index):
#         x = self.gcn1(x, edge_index)
#         x = self.gcn2(x, edge_index)
#         return self.mlp(x)

# # === Ë©ï‰æ°Èñ¢Êï∞ ===
# def evaluate():
#     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#     all_preds, all_labels = [], []

#     for fname in os.listdir(graph_dir):
#         if not fname.endswith("_test.pt"): continue
#         dtype = fname.replace("graph_", "").replace("_test.pt", "")
#         print(f"\nüß™ Evaluating test graph for disaster type: {dtype}")

#         graph_path = os.path.join(graph_dir, fname)
#         model_path = os.path.join(model_dir, f"gcn_{dtype}.pt")
#         if not os.path.exists(model_path):
#             print(f"‚ö†Ô∏è Model not found for {dtype}, skipping.")
#             continue

#         # „Ç∞„É©„Éï„Å®„É¢„Éá„É´Ë™≠„ÅøËæº„Åø
#         g = torch.load(graph_path, weights_only=False)
#         model = GCNClassifier().to(device)
#         model.load_state_dict(torch.load(model_path, map_location=device))
#         model.eval()

#         x = g.x.to(device)
#         edge_index = g.edge_index.to(device)
#         y_true = g.y.cpu().numpy()

#         with torch.no_grad():
#             out = model(x, edge_index)
#             y_pred = out.argmax(dim=1).cpu().numpy()

#         acc = accuracy_score(y_true, y_pred)
#         print(f"‚úÖ Accuracy: {acc:.4f}")
#         all_preds.extend(y_pred)
#         all_labels.extend(y_true)

#     # === ÂÖ®‰ΩìË©ï‰æ° ===
#     if len(all_preds) == 0:
#         print("‚ö†Ô∏è No predictions made.")
#     else:
#         print("\nüìä Overall GCN Test Evaluation (test-only graphs):")
#         print("‚úÖ Accuracy:", accuracy_score(all_labels, all_preds))
#         print("‚úÖ Confusion Matrix:\n", confusion_matrix(all_labels, all_preds))
#         print("‚úÖ Classification Report:\n",
#               classification_report(all_labels, all_preds, target_names=["no-damage", "minor", "major", "destroyed"]))

# # === ÂÆüË°å ===
# if __name__ == "__main__":
#     evaluate()

