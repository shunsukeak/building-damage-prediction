import os
import torch
import torch.nn as nn
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from tqdm import tqdm

# === ÂÖ•Âá∫Âäõ„Éë„Çπ ===
graph_dir = "./graphs_test_only"
model_dir = "./gcn_models_by_type"

# === GCN„É¢„Éá„É´ÂÆöÁæ© ===
class GCNClassifier(nn.Module):
    def __init__(self, in_dim=518, hidden_dim=128, out_dim=4):
        super().__init__()
        self.gcn1 = GCNConv(in_dim, hidden_dim)
        self.gcn2 = GCNConv(hidden_dim, hidden_dim)
        self.mlp = nn.Sequential(
            nn.ReLU(),
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Linear(64, out_dim)
        )

    def forward(self, x, edge_index):
        x = self.gcn1(x, edge_index)
        x = self.gcn2(x, edge_index)
        return self.mlp(x)

# === Ë©ï‰æ°Èñ¢Êï∞ ===
def evaluate():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    all_preds, all_labels = [], []

    for fname in os.listdir(graph_dir):
        if not fname.endswith("_test.pt"): continue
        dtype = fname.replace("graph_", "").replace("_test.pt", "")
        print(f"\nüß™ Evaluating test graph for disaster type: {dtype}")

        graph_path = os.path.join(graph_dir, fname)
        model_path = os.path.join(model_dir, f"gcn_{dtype}.pt")
        if not os.path.exists(model_path):
            print(f"‚ö†Ô∏è Model not found for {dtype}, skipping.")
            continue

        # „Ç∞„É©„Éï„Å®„É¢„Éá„É´Ë™≠„ÅøËæº„Åø
        g = torch.load(graph_path, weights_only=False)
        model = GCNClassifier().to(device)
        model.load_state_dict(torch.load(model_path, map_location=device))
        model.eval()

        x = g.x.to(device)
        edge_index = g.edge_index.to(device)
        y_true = g.y.cpu().numpy()

        with torch.no_grad():
            out = model(x, edge_index)
            y_pred = out.argmax(dim=1).cpu().numpy()

        acc = accuracy_score(y_true, y_pred)
        print(f"‚úÖ Accuracy: {acc:.4f}")
        all_preds.extend(y_pred)
        all_labels.extend(y_true)

    # === ÂÖ®‰ΩìË©ï‰æ° ===
    if len(all_preds) == 0:
        print("‚ö†Ô∏è No predictions made.")
    else:
        print("\nüìä Overall GCN Test Evaluation (test-only graphs):")
        print("‚úÖ Accuracy:", accuracy_score(all_labels, all_preds))
        print("‚úÖ Confusion Matrix:\n", confusion_matrix(all_labels, all_preds))
        print("‚úÖ Classification Report:\n",
              classification_report(all_labels, all_preds, target_names=["no-damage", "minor", "major", "destroyed"]))

# === ÂÆüË°å ===
if __name__ == "__main__":
    evaluate()

